# MÃ¤ngelmelder Konstanz - NLP-Analyse

**Projekt: Data Analysis (DLBDSEDA02_D)**  
**Autor: Tobias Seekatz**  
**Sprache: Deutsch (NLP fÃ¼r deutsche Texte)**

Automatisierte Identifikation von Problemthemen aus BÃ¼rgerbeschwerden mittels Natural Language Processing (NLP). Das System analysiert unstrukturierte Texteingaben von BÃ¼rgern und extrahiert automatisch semantische Themen, hÃ¤ufig vorkommende Wortmuster und visualisiert diese interaktiv.

---

## ğŸ¯ Zielsetzung

Das Projekt lÃ¶st das Problem der manuellen Kategorisierung von tausenden BÃ¼rgerbeschwerden in der Stadt Konstanz. Mittels moderner NLP-Techniken werden:

- **Automatische Themenextraktion**: Identifiziert 5-N Hauptthemen aus unstrukturierten Texten
- **Kontextuelle Wortmuster**: Findet wiederkehrende Bigramme (z.B. â€StraÃŸenlaterne defekt")
- **Semantische Analyse**: Versteht Umlaute, Varianten und deutsche Grammatik
- **Interaktive Exploration**: Erlaubt explorative Datenanalyse Ã¼ber pyLDAvis Dashboard

### Beispiel-Erkenntnisse aus den Daten:
```
Top-Thema 1: Illegale MÃ¼llablagerung, abgestellte Fahrzeuge
Top-Thema 2: GehwegschÃ¤den, Baumverschmutzung
Top-Thema 3: Radweginstandhaltung, StraÃŸenbeleuchtung
Top-Thema 4: FuÃŸgÃ¤nger-/Radfahrer-Sicherheit
Top-Thema 5: Defekte StraÃŸenlaternen (hÃ¤ufigste Beschwerde: 377x Wort â€defekt")
```

## âœ¨ Features

### 1. **Datenvorverarbeitung (Text Cleaning)**
- Unicode-Normalisierung (NFC) fÃ¼r deutsche Umlaute (Ã¤, Ã¶, Ã¼, ÃŸ)
- Entfernung von Satzzeichen & Zahlen
- Automatische Stoppwort-Filterung (195 deutsche StoppwÃ¶rter)
- **Lemmatisierung**: spaCy-Modell `de_core_news_sm` mit NLTK-Fallback
- **Auto-Download**: Modell wird automatisch heruntergeladen falls nicht vorhanden

### 2. **Vektorisierung**
- **CountVectorizer**: Bag-of-Words (BoW) Darstellung
- **TF-IDF**: Term Frequency-Inverse Document Frequency fÃ¼r gewichtete Features
- Maximale Features: 500 (konfigurierbar)
- Dokumentfrequenz-Filterung: min 2 Dokumente, max 95%

### 3. **Topic Modeling (LDA)**
- **Latent Dirichlet Allocation** mit scikit-learn
- 5 Themen (konfigurierbar via `--topics` Flag)
- 20 Iterationen (bis zur Konvergenz)
- Extraktion der Top-10 WÃ¶rter pro Thema

### 4. **N-Gramm-Analyse**
- Bigramme (2er-Wortpaare) werden extrahiert
- Top-20 Bigramme mit HÃ¤ufigkeitszÃ¤hler
- ErmÃ¶glicht Erkennung von hÃ¤ufigen Phrasen (z.B. â€StraÃŸenlaterne defekt")

### 5. **Visualisierungen**
- **WordCloud**: GrÃ¶ÃŸe der WÃ¶rter proportional zur HÃ¤ufigkeit
- **Top-WÃ¶rter Bar-Plot**: Horizontales Balkendiagramm der 20 hÃ¤ufigsten WÃ¶rter
- **pyLDAvis HTML-Dashboard**: Interaktive 2D-Themenvisualisierung (t-SNE Reduktion)
  - Klicke auf Themen-Blasen zum Erkunden
  - Sieh relevante WÃ¶rter pro Thema
  - Themenverteilung Ã¼ber alle Dokumente

## ğŸš€ Schnellstart

### Installation (5 Minuten)

#### Option 1: Mit Skript (empfohlen)
```bash
# Klone das Repository
git clone https://github.com/Toby2906/Maengelmelder-Konstanz.git
cd Maengelmelder-Konstanz

# Erstelle Virtual Environment
python3 -m venv .venv
source .venv/bin/activate  # Mac/Linux
# oder: .venv\Scripts\activate  # Windows

# Installiere Dependencies
pip install --upgrade pip
pip install -r requirements.txt

# (Optional) Manuelle spaCy-Installation (normalerweise nicht nÃ¶tig!)
# python -m spacy download de_core_news_sm
```

#### Option 2: Mit Docker
```bash
docker build -t maengelmelder-nlp .
docker run -v $(pwd)/data:/app/data maengelmelder-nlp python main.py
```

### Daten vorbereiten

1. CSV-Datei mit BÃ¼rgerbeschwerden (UTF-8 encoding):
   ```csv
   ID,Kategorie,Beschreibung
   1,MÃ¼ll,"Illegale MÃ¼llablagerung am Hafenplatz"
   2,StraÃŸe,"StraÃŸenlaterne vor Haus 42 ist defekt"
   ...
   ```

2. Kopiere in: `data/raw/maengelmelder_konstanz.csv`

### Analyse starten

```bash
# Standard: 5 Themen
python main.py

# Mit benutzerdefinierten Parametern
python main.py --input data/raw/custom_data.csv --topics 7

# Debug-Modus (verbose logging)
python main.py --verbose
```

### Ergebnisse anschauen

```bash
# Statische Visualisierungen
ls output/visualizations/
# â†’ wordcloud.png          (Wort-HÃ¤ufigkeit)
# â†’ top_words.png          (Top-20 WÃ¶rter)

# Interaktives Dashboard Ã¶ffnen
cd output/reports
python -m http.server 8000
# Browser: http://localhost:8000/lda_interactive.html

# Rohe Ergebnisse (JSON)
cat output/analysis/analysis_results.json
```

## ğŸ“Š Verwendung & Konfiguration

### Kommandozeilen-Argumente

```bash
# Alle verfÃ¼gbaren Optionen
python main.py --help

# Beispiele:
python main.py --topics 10                              # 10 statt 5 Themen
python main.py --input my_data.csv --topics 7          # Custom CSV + 7 Themen
python main.py --verbose                               # Debug-Output
```

### Konfiguration Ã¤ndern (src/config.py)

```python
# Text-Vorverarbeitung
MIN_WORD_LENGTH = 3          # Nur WÃ¶rter â‰¥ 3 Zeichen
REMOVE_PUNCTUATION = True    # Satzzeichen entfernen
USE_LEMMATIZATION = True     # Lemmatisierung ein/aus

# Vektorisierung
VECTORIZER_MAX_FEATURES = 500
VECTORIZER_MIN_DF = 2        # Min. 2 Dokumente pro Token

# Topic Modeling
LDA_N_TOPICS = 5
LDA_MAX_ITER = 20

# Visualisierung
WORDCLOUD_MAX_WORDS = 100
PLOT_DPI = 150              # BildauflÃ¶sung
```

### Wichtige Hinweise

> âš ï¸ **spaCy-Modell**: Fehlt `de_core_news_sm`, wird automatisch heruntergeladen. Falls das fehlschlÃ¤gt:
> ```bash
> python -m spacy download de_core_news_sm
> ```
> Fallback: NLTK SnowballStemmer wird als Lemmatizer verwendet.

> ğŸ’¡ **pyLDAvis**: Optional fÃ¼r interaktive HTML-Visualisierung. Falls fehlend:
> ```bash
> pip install pyLDAvis
> ```
> Ohne pyLDAvis: Analyse lÃ¤uft normal, nur kein interaktives Dashboard.

> ğŸ“ˆ **Performance**: Bei >5000 Texten dauert LDA lÃ¤nger. Nutze `--topics 3` fÃ¼r schnellere Tests.

---

## ğŸ› ï¸ Architektur & Module

### Dateistruktur

```
Maengelmelder-Konstanz/
â”‚
â”œâ”€â”€ main.py                    # Einstiegspunkt, orchestriert Pipeline
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py              # Zentrale Konfiguration
â”‚   â”œâ”€â”€ data_loader.py         # CSV-Laden & Basis-Statistiken
â”‚   â”œâ”€â”€ preprocessor.py        # Text-Cleaning, Lemmatisierung
â”‚   â”œâ”€â”€ vectorizer.py          # CountVectorizer & TF-IDF
â”‚   â”œâ”€â”€ topic_modeler.py       # LDA-Training
â”‚   â”œâ”€â”€ analyzer.py            # N-Gramm, Top-WÃ¶rter
â”‚   â””â”€â”€ visualizer.py          # WordCloud, Plots, pyLDAvis
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                   # Eingabe (CSV)
â”‚   â””â”€â”€ processed/             # TemporÃ¤re Outputs
â”‚
â”œâ”€â”€ resources/
â”‚   â””â”€â”€ stopwords_de.txt       # 195 deutsche StoppwÃ¶rter
â”‚
â”œâ”€â”€ output/
â”‚   â”œâ”€â”€ visualizations/        # WordCloud, Plots (PNG)
â”‚   â”œâ”€â”€ reports/               # pyLDAvis HTML
â”‚   â””â”€â”€ analysis/              # JSON mit Ergebnissen
â”‚
â”œâ”€â”€ requirements.txt           # Python-Dependencies
â””â”€â”€ README.md                  # Diese Datei
```

### Pipeline-Flow

```
1. Daten laden (CSV) â†’ 2132 Texte
      â†“
2. Vorverarbeitung (spaCy/NLTK) â†’ 2127 saubere Texte, 5109 Tokens
      â†“
3. Vektorisierung (CountVectorizer) â†’ 2127Ã—500 Matrix
      â†“
4. LDA Topic Modeling â†’ 5 Themen
      â†“
5. N-Gramm Analyse â†’ Top-20 Bigramme
      â†“
6. Visualisierungen & Export â†’ PNG + HTML + JSON
```

---

## ğŸ”§ Software-Stack Detailliert

| Komponente | Paket | Version | Zweck |
|-----------|-------|---------|-------|
| NLP | `spacy` | 3.7+ | Lemmatisierung, Tokenisierung |
| NLP (Fallback) | `nltk` | 3.8+ | Stemming (wenn spaCy fehlt) |
| ML | `scikit-learn` | 1.3+ | Vektorisierung, LDA |
| Daten | `pandas` | 2.0+ | CSV-Verarbeitung, Statistiken |
| Visualisierung | `matplotlib` | 3.8+ | Plots |
| Visualisierung | `wordcloud` | 1.9+ | WordCloud-Erstellung |
| Visualisierung | `pyLDAvis` | 3.4+ | Interaktive Topic-Visualisierung |
| Datenverarbeitung | `numpy` | 1.24+ | Numerische Operationen |
| Datenverarbeitung | `scipy` | 1.17+ | Wissenschaftliche Funktionen |

---

## ğŸ“ˆ Beispiel-Output

### Analyse-Ergebnisse (analysis_results.json)
```json
{
  "data_stats": {
    "total_texts": 2132,
    "avg_words": 18.4,
    "avg_length": 133.7
  },
  "preprocessing_stats": {
    "total_texts": 2127,
    "unique_tokens": 5109,
    "avg_tokens_per_text": 9.4
  },
  "topics": {
    "Thema 1": ["steht", "mÃ¼ll", "strasse", "liegt", "abgestellt", ...],
    "Thema 2": ["schild", "gehweg", "baum", "liegt", "fussweg", ...],
    ...
  },
  "top_bigrams": {
    "strassenlaterne defekt": 54,
    "lamp defekt": 42,
    "laterne defekt": 32,
    ...
  },
  "top_words": {
    "defekt": 377,
    "strasse": 251,
    "strassenlaterne": 236,
    ...
  }
}
```

### Visualisierungen
- `wordcloud.png`: Visuelle Darstellung der Wort-HÃ¤ufigkeiten
- `top_words.png`: Top-20 WÃ¶rter als Balkendiagramm
- `lda_interactive.html`: Interaktives pyLDAvis Dashboard (Ã¶ffne im Browser)

---

## ğŸ› Troubleshooting

| Problem | LÃ¶sung |
|---------|--------|
| `ModuleNotFoundError: No module named 'spacy'` | `pip install -r requirements.txt` |
| `OSError: No such file or directory: 'data/raw/...'` | PrÃ¼fe CSV-Pfad, muss UTF-8 sein |
| `Empty vocabulary after vectorization` | Stoppwort-Filter zu aggressiv? PrÃ¼fe `config.py` |
| `pyLDAvis nicht verfÃ¼gbar` | `pip install pyLDAvis` |
| `spaCy Modell 'de_core_news_sm' nicht gefunden` | Auto-Download startet, oder manuell: `python -m spacy download de_core_news_sm` |
| Analyse ist sehr langsam | Reduziere `VECTORIZER_MAX_FEATURES` oder nutze `--topics 3` |

---

## ğŸ“š Ressourcen & Dokumentation

- **spaCy Docs**: https://spacy.io
- **scikit-learn LDA**: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html
- **pyLDAvis Docs**: https://pyldavis.readthedocs.io/
- **Deutsche StoppwÃ¶rter**: `resources/stopwords_de.txt` (195 WÃ¶rter)

---

## ğŸ“ Lizenz & Autor

**Autor**: Tobias Seekatz  
**Projekt**: Data Analysis (DLBDSEDA02_D)  
**Datum**: Januar 2026  
**Lizenz**: MIT (siehe LICENSE Datei)

---

## ğŸš€ Weitere Verbesserungen (Roadmap)

- [ ] Gensim LDA fÃ¼r bessere Performance bei groÃŸen Datasets
- [ ] ThemenverÃ¤nderung Ã¼ber Zeit analysieren
- [ ] Export in andere Formate (XML, CSV mit Zuordnung)
- [ ] REST-API fÃ¼r Echtzeit-Klassifizierung
- [ ] Mehrsprachigkeit (Englisch, FranzÃ¶sisch)
- [ ] Sentiment-Analyse der Beschwerden

---

## ğŸ› ï¸ Basis-Software-Stack

- **Python 3.8+**
- **pandas**: Datenmanagement
- **spaCy**: Deutsches NLP & Lemmatisierung
- **scikit-learn**: Vektorisierung & LDA
- **matplotlib**: Visualisierung
- **wordcloud**: WordCloud-Erstellung
- **pyLDAvis**: Interaktive Topic-Visualisierung

## ğŸ“ Projekt-Ãœbersicht

Maengelmelder-Konstanz/
â”œâ”€â”€ src/                 # Hauptcode
â”œâ”€â”€ data/raw/            # CSV-Daten
â”œâ”€â”€ output/              # Ergebnisse
â”œâ”€â”€ resources/           # StoppwÃ¶rter
â”œâ”€â”€ main.py              # Einstiegspunkt
â””â”€â”€ README.md

